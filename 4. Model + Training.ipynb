{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a23a87fd",
   "metadata": {},
   "source": [
    "Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b877e78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data splitting with stratification and subject-wise splitting\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "# 70% Train, 20% Validation, 10% Test\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.1, random_state=42)\n",
    "train_val_idx, test_idx = next(sss.split(X, y))\n",
    "X_temp, X_test = X[train_val_idx], X[test_idx]\n",
    "y_temp, y_test = y[train_val_idx], y[test_idx]\n",
    "\n",
    "# split train_val (X_temp, y_temp) into 70% train and 20% val (of original)\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.22, random_state=42)  # 0.22 * 0.9 â‰ˆ 0.2 overall\n",
    "train_idx, val_idx = next(sss.split(X_temp, y_temp))\n",
    "X_train, X_val = X_temp[train_idx], X_temp[val_idx]\n",
    "y_train, y_val = y_temp[train_idx], y_temp[val_idx]\n",
    "\n",
    "# Convert to tensors\n",
    "X_train = torch.FloatTensor(X_train).unsqueeze(1) \n",
    "X_val = torch.FloatTensor(X_val).unsqueeze(1)\n",
    "X_test = torch.FloatTensor(X_test).unsqueeze(1)\n",
    "y_train = torch.LongTensor(y_train)\n",
    "y_val = torch.LongTensor(y_val)\n",
    "y_test = torch.LongTensor(y_test)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "val_dataset = TensorDataset(X_val, y_val)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "print(\"Enhanced data split completed:\")\n",
    "print(f\"Train: {len(train_dataset)}, Val: {len(val_dataset)}, Test: {len(test_dataset)}\")\n",
    "print(f\"X_train shape: {X_train.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d74cf6",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b362bf15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EEGNet + attention and residual connections\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        \n",
    "        self.qkv = nn.Linear(embed_dim, embed_dim * 3, bias=False)\n",
    "        self.proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        x = x.flatten(2).transpose(1, 2)  # B, HW, C\n",
    "        \n",
    "        qkv = self.qkv(x).reshape(B, -1, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        \n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        \n",
    "        out = (attn @ v).transpose(1, 2).reshape(B, -1, C)\n",
    "        out = self.proj(out)\n",
    "        \n",
    "        return out.transpose(1, 2).reshape(B, C, H, W)\n",
    "\n",
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, in_channels, reduction=16):\n",
    "        super().__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, in_channels // reduction, 1, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels // reduction, in_channels, 1, bias=False)\n",
    "        )\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        avg_out = self.fc(self.avg_pool(x))\n",
    "        max_out = self.fc(self.max_pool(x))\n",
    "        out = avg_out + max_out\n",
    "        return x * self.sigmoid(out)\n",
    "\n",
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self, kernel_size=7):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(2, 1, kernel_size, padding=kernel_size//2, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        x_cat = torch.cat([avg_out, max_out], dim=1)\n",
    "        x_cat = self.conv(x_cat)\n",
    "        return x * self.sigmoid(x_cat)\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, stride, 1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, 1, stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        return F.relu(out)\n",
    "\n",
    "class StateOfTheArtEEGNet(nn.Module):\n",
    "    def __init__(self, input_shape, num_classes=4, dropout_rate=0.25, F1=32, D=2, F2=64):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_channels, self.input_time_points = input_shape\n",
    "        self.F1 = F1\n",
    "        self.D = D\n",
    "        self.F2 = F2\n",
    "        \n",
    "        # Block 1: Temporal Convolution with residual connections\n",
    "        self.conv1 = nn.Conv2d(1, F1, (1, 64), padding=(0, 32), bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(F1)\n",
    "        \n",
    "        # Block 2: Depthwise Convolution with attention\n",
    "        self.conv2 = nn.Conv2d(F1, D * F1, (self.input_channels, 1), groups=F1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(D * F1)\n",
    "        self.elu1 = nn.ELU()\n",
    "        self.avgpool1 = nn.AvgPool2d((1, 4))\n",
    "        self.dropout1 = nn.Dropout2d(dropout_rate)\n",
    "        \n",
    "        # Channel attention\n",
    "        self.channel_att1 = ChannelAttention(D * F1)\n",
    "        \n",
    "        # Block 3: Separable Convolution\n",
    "        self.conv3 = nn.Conv2d(D * F1, D * F1, (1, 16), padding=(0, 8), groups=D * F1, bias=False)\n",
    "        self.conv4 = nn.Conv2d(D * F1, F2, (1, 1), bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(F2)\n",
    "        self.elu2 = nn.ELU()\n",
    "        self.avgpool2 = nn.AvgPool2d((1, 8))\n",
    "        self.dropout2 = nn.Dropout2d(dropout_rate)\n",
    "        \n",
    "        # Additional residual blocks\n",
    "        self.res_block1 = ResidualBlock(F2, F2)\n",
    "        self.res_block2 = ResidualBlock(F2, F2)\n",
    "        \n",
    "        # Spatial attention\n",
    "        self.spatial_att = SpatialAttention()\n",
    "        \n",
    "        # Multi-head attention\n",
    "        self.mha = MultiHeadAttention(F2, num_heads=4)\n",
    "        \n",
    "        # Global average pooling\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        \n",
    "        # classifier with multiple branches\n",
    "        feature_size = F2\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(feature_size, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "        \n",
    "        # Auxiliary classifier for regularization\n",
    "        self.aux_classifier = nn.Sequential(\n",
    "            nn.Linear(feature_size, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate * 0.5),\n",
    "            nn.Linear(32, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Block 1\n",
    "        x = self.bn1(self.conv1(x))\n",
    "        \n",
    "        # Block 2\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.elu1(x)\n",
    "        x = self.avgpool1(x)\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        # Apply channel attention\n",
    "        x = self.channel_att1(x)\n",
    "        \n",
    "        # Block 3\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.elu2(x)\n",
    "        x = self.avgpool2(x)\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        # Residual blocks\n",
    "        x = self.res_block1(x)\n",
    "        x = self.res_block2(x)\n",
    "        \n",
    "        # Apply spatial attention\n",
    "        x = self.spatial_att(x)\n",
    "        \n",
    "        # Multi-head attention\n",
    "        x = self.mha(x)\n",
    "        \n",
    "        # Global average pooling\n",
    "        features = self.global_avg_pool(x)\n",
    "        features = features.view(features.size(0), -1)\n",
    "        \n",
    "        # Main classifier\n",
    "        main_output = self.classifier(features)\n",
    "        \n",
    "        # Auxiliary classifier (for training regularization)\n",
    "        aux_output = self.aux_classifier(features)\n",
    "        \n",
    "        return main_output, aux_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54bc580a",
   "metadata": {},
   "source": [
    "Trainer Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680fda1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training with multiple loss functions and optimization\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2, reduction='mean'):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "        \n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss\n",
    "        \n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss\n",
    "\n",
    "class LabelSmoothingLoss(nn.Module):\n",
    "    def __init__(self, num_classes, smoothing=0.1):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.smoothing = smoothing\n",
    "        \n",
    "    def forward(self, pred, target):\n",
    "        with torch.no_grad():\n",
    "            smooth_target = torch.zeros_like(pred)\n",
    "            smooth_target.fill_(self.smoothing / (self.num_classes - 1))\n",
    "            smooth_target.scatter_(1, target.unsqueeze(1), 1.0 - self.smoothing)\n",
    "        \n",
    "        return F.kl_div(F.log_softmax(pred, dim=1), smooth_target, reduction='batchmean')\n",
    "\n",
    "class ATrainer:\n",
    "    def __init__(self, model, device, num_classes=4):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # Multiple loss functions\n",
    "        self.criterion_ce = nn.CrossEntropyLoss()\n",
    "        self.criterion_focal = FocalLoss(alpha=1, gamma=2)\n",
    "        self.criterion_smooth = LabelSmoothingLoss(num_classes, smoothing=0.1)\n",
    "        \n",
    "        # Optimizers with different learning rates\n",
    "        self.optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)\n",
    "        self.scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "            self.optimizer, T_0=10, T_mult=2, eta_min=1e-6\n",
    "        )\n",
    "        \n",
    "        # Early stopping\n",
    "        self.best_val_acc = 0\n",
    "        self.patience = 15\n",
    "        self.patience_counter = 0\n",
    "        \n",
    "        # Mixed precision training\n",
    "        self.scaler = torch.cuda.amp.GradScaler() if device.type == 'cuda' else None\n",
    "        \n",
    "    def train_epoch(self, train_loader, epoch):\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(self.device), target.to(self.device)\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            if self.scaler:\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    main_output, aux_output = self.model(data)\n",
    "                    \n",
    "                    # Combined loss\n",
    "                    loss_ce = self.criterion_ce(main_output, target)\n",
    "                    loss_focal = self.criterion_focal(main_output, target)\n",
    "                    loss_smooth = self.criterion_smooth(main_output, target)\n",
    "                    aux_loss = self.criterion_ce(aux_output, target)\n",
    "                    \n",
    "                    loss = 0.4 * loss_ce + 0.3 * loss_focal + 0.2 * loss_smooth + 0.1 * aux_loss\n",
    "                \n",
    "                self.scaler.scale(loss).backward()\n",
    "                self.scaler.step(self.optimizer)\n",
    "                self.scaler.update()\n",
    "            else:\n",
    "                main_output, aux_output = self.model(data)\n",
    "                \n",
    "                loss_ce = self.criterion_ce(main_output, target)\n",
    "                loss_focal = self.criterion_focal(main_output, target)\n",
    "                loss_smooth = self.criterion_smooth(main_output, target)\n",
    "                aux_loss = self.criterion_ce(aux_output, target)\n",
    "                \n",
    "                loss = 0.4 * loss_ce + 0.3 * loss_focal + 0.2 * loss_smooth + 0.1 * aux_loss\n",
    "                \n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "                self.optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            pred = main_output.argmax(dim=1)\n",
    "            correct += pred.eq(target).sum().item()\n",
    "            total += target.size(0)\n",
    "            \n",
    "            if batch_idx % 50 == 0:\n",
    "                print(f'Epoch {epoch}, Batch {batch_idx}, Loss: {loss.item():.4f}, '\n",
    "                      f'Acc: {100.*correct/total:.2f}%')\n",
    "        \n",
    "        return total_loss / len(train_loader), 100. * correct / total\n",
    "    \n",
    "    def validate(self, val_loader):\n",
    "        self.model.eval()\n",
    "        val_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for data, target in val_loader:\n",
    "                data, target = data.to(self.device), target.to(self.device)\n",
    "                main_output, _ = self.model(data)\n",
    "                \n",
    "                val_loss += self.criterion_ce(main_output, target).item()\n",
    "                pred = main_output.argmax(dim=1)\n",
    "                correct += pred.eq(target).sum().item()\n",
    "                total += target.size(0)\n",
    "        \n",
    "        val_acc = 100. * correct / total\n",
    "        return val_loss / len(val_loader), val_acc\n",
    "    \n",
    "    def train(self, train_loader, val_loader, epochs=100):\n",
    "        train_losses, train_accs = [], []\n",
    "        val_losses, val_accs = [], []\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Training\n",
    "            train_loss, train_acc = self.train_epoch(train_loader, epoch)\n",
    "            train_losses.append(train_loss)\n",
    "            train_accs.append(train_acc)\n",
    "            \n",
    "            # Validation\n",
    "            val_loss, val_acc = self.validate(val_loader)\n",
    "            val_losses.append(val_loss)\n",
    "            val_accs.append(val_acc)\n",
    "            \n",
    "            # Learning rate scheduling\n",
    "            self.scheduler.step()\n",
    "            \n",
    "            print(f'Epoch {epoch}: Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, '\n",
    "                  f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n",
    "            \n",
    "            # Early stopping\n",
    "            if val_acc > self.best_val_acc:\n",
    "                self.best_val_acc = val_acc\n",
    "                self.patience_counter = 0\n",
    "                torch.save(self.model.state_dict(), 'best_model.pth')\n",
    "            else:\n",
    "                self.patience_counter += 1\n",
    "                if self.patience_counter >= self.patience:\n",
    "                    print(f'Early stopping at epoch {epoch}')\n",
    "                    break\n",
    "        \n",
    "        return {\n",
    "            'train_losses': train_losses,\n",
    "            'train_accs': train_accs,\n",
    "            'val_losses': val_losses,\n",
    "            'val_accs': val_accs\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2742716b",
   "metadata": {},
   "source": [
    "Model Initialization and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2eb01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model initialization and training\n",
    "input_shape = (X_train.shape[2], X_train.shape[3])  # (channels, time_points)\n",
    "model = StateOfTheArtEEGNet(input_shape, num_classes=4, dropout_rate=0.3, F1=64, D=2, F2=128).to(device)\n",
    "\n",
    "# Print model summary\n",
    "print(\"Model architecture:\")\n",
    "print(model)\n",
    "\n",
    "# Calculate total parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'\\nTotal parameters: {total_params:,}')\n",
    "print(f'Trainable parameters: {trainable_params:,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46bffca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loading with weighted sampling for class imbalance\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "\n",
    "# Calculate class weights for balanced sampling\n",
    "class_counts = np.bincount(y_train.cpu().numpy())\n",
    "class_weights = 1.0 / class_counts\n",
    "sample_weights = class_weights[y_train.cpu().numpy()]\n",
    "\n",
    "sampler = WeightedRandomSampler(\n",
    "    weights=sample_weights,\n",
    "    num_samples=len(sample_weights),\n",
    "    replacement=True\n",
    ")\n",
    "\n",
    "# Create data loaders with optimized parameters\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=sampler, \n",
    "                         num_workers=0, pin_memory=True)\n",
    "\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, \n",
    "                       num_workers=0, pin_memory=True)\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, \n",
    "                        num_workers=0, pin_memory=True)\n",
    "\n",
    "\n",
    "print(f\"Data loaders created:\")\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Val batches: {len(val_loader)}\")\n",
    "print(f\"Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d60edb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "trainer = ATrainer(model, device, num_classes=4)\n",
    "\n",
    "print(\"Starting advanced training...\")\n",
    "history = trainer.train(train_loader, val_loader, epochs=80)\n",
    "\n",
    "# Load best model\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "print(f\"Best validation accuracy: {trainer.best_val_acc:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
