{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c5d1604",
   "metadata": {},
   "source": [
    "Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0dcdee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation\n",
    "class EEGAugmentation:\n",
    "    def __init__(self, noise_std=0.01, time_shift_max=50, freq_shift_max=2):\n",
    "        self.noise_std = noise_std\n",
    "        self.time_shift_max = time_shift_max\n",
    "        self.freq_shift_max = freq_shift_max\n",
    "    \n",
    "    def add_gaussian_noise(self, data, std=None):\n",
    "        if std is None:\n",
    "            std = self.noise_std\n",
    "        noise = np.random.normal(0, std, data.shape)\n",
    "        return data + noise\n",
    "    \n",
    "    def time_shift(self, data):\n",
    "        shift = np.random.randint(-self.time_shift_max, self.time_shift_max)\n",
    "        if shift > 0:\n",
    "            return np.concatenate([data[:, shift:], data[:, :shift]], axis=1)\n",
    "        elif shift < 0:\n",
    "            return np.concatenate([data[:, shift:], data[:, :shift]], axis=1)\n",
    "        return data\n",
    "    \n",
    "    def amplitude_scale(self, data, scale_range=(0.75, 1.25)):\n",
    "        scale = np.random.uniform(scale_range[0], scale_range[1])\n",
    "        return data * scale\n",
    "    \n",
    "    def frequency_shift(self, data, fs=250, max_shift=2):\n",
    "        fft_data = np.fft.fft(data, axis=1)\n",
    "        shift = np.random.randint(-max_shift, max_shift + 1)\n",
    "        if shift != 0:\n",
    "            fft_data = np.roll(fft_data, shift, axis=1)\n",
    "        return np.real(np.fft.ifft(fft_data, axis=1))\n",
    "    \n",
    "    def channel_dropout(self, data, dropout_prob=0.1):\n",
    "        mask = np.random.random(data.shape[0]) > dropout_prob\n",
    "        augmented_data = data.copy()\n",
    "        augmented_data[~mask] = 0\n",
    "        return augmented_data\n",
    "    \n",
    "    def mixup(self, data1, data2, alpha=0.2):\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "        return lam * data1 + (1 - lam) * data2\n",
    "    \n",
    "    def time_warping(self, data, sigma=0.2):\n",
    "        time_steps = data.shape[1]\n",
    "        random_warps = np.random.normal(size=time_steps) * sigma\n",
    "        cumulative_warps = np.cumsum(random_warps)\n",
    "        warped_indices = np.clip(np.arange(time_steps) + cumulative_warps, 0, time_steps - 1).astype(int)\n",
    "        return data[:, warped_indices]\n",
    "    \n",
    "    def augment_batch(self, data, p=0.5):\n",
    "        augmented_data = data.copy()\n",
    "        batch_size = len(data)\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            if np.random.random() < p:\n",
    "                # Apply random combination of augmentations\n",
    "                if np.random.random() < 0.4:\n",
    "                    augmented_data[i] = self.add_gaussian_noise(augmented_data[i])\n",
    "                if np.random.random() < 0.3:\n",
    "                    augmented_data[i] = self.time_shift(augmented_data[i])\n",
    "                if np.random.random() < 0.3:\n",
    "                    augmented_data[i] = self.amplitude_scale(augmented_data[i])\n",
    "                if np.random.random() < 0.2:\n",
    "                    augmented_data[i] = self.frequency_shift(augmented_data[i])\n",
    "                if np.random.random() < 0.1:\n",
    "                    augmented_data[i] = self.channel_dropout(augmented_data[i])\n",
    "                if np.random.random() < 0.2:\n",
    "                    augmented_data[i] = self.time_warping(augmented_data[i])\n",
    "                \n",
    "        return augmented_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c4ff83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation\n",
    "augmenter = EEGAugmentation(noise_std=0.015, time_shift_max=25)\n",
    "\n",
    "augmented_versions = []\n",
    "for _ in range(3):  # Create 3 augmented versions\n",
    "    X_aug = augmenter.augment_batch(X_first_session, p=0.6)\n",
    "    augmented_versions.append(X_aug)\n",
    "\n",
    "# Combine augmented data\n",
    "X_train_combined = np.concatenate([X_first_session] + augmented_versions)\n",
    "y_train_combined = np.concatenate([y_train] * (1 + len(augmented_versions)))\n",
    "\n",
    "print(f\"Combined training data shape: {X_train_combined.shape}\")\n",
    "\n",
    "# Final dataset\n",
    "X = np.concatenate((X_train_combined, X_second_session))\n",
    "y = np.concatenate((y_train_combined, y_test))\n",
    "\n",
    "print(f\"Total dataset shape: {X.shape}\")\n",
    "print(f\"Label distribution: {np.bincount(y.astype(int))}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
